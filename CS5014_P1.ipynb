{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a705a7",
   "metadata": {},
   "source": [
    "# CS5014 Machine Learning \n",
    "\n",
    "##### Practical 1 \n",
    "##### Credits: 50% of the coursework\n",
    "##### Deadline: 12/03/2025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662dc6a",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "You are **only allowed** to use the following imported packages for this practical. No off-the-shelf machine learning packages such as _scikit-learn_ are allowed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931f3a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:15.361137Z",
     "iopub.status.busy": "2025-02-13T12:59:15.360644Z",
     "iopub.status.idle": "2025-02-13T12:59:16.727737Z",
     "shell.execute_reply": "2025-02-13T12:59:16.726720Z",
     "shell.execute_reply.started": "2025-02-13T12:59:15.361084Z"
    }
   },
   "outputs": [],
   "source": [
    "# if you use jupyter-lab, switch to %matplotlib inline instead\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "%config Completer.use_jedi = False\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad    \n",
    "from autograd import hessian\n",
    "import autograd.numpy.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedfaf21-f585-47e4-a083-df8d9bd467ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.729201Z",
     "iopub.status.busy": "2025-02-13T12:59:16.728799Z",
     "iopub.status.idle": "2025-02-13T12:59:16.734994Z",
     "shell.execute_reply": "2025-02-13T12:59:16.734251Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.729173Z"
    }
   },
   "outputs": [],
   "source": [
    "def finite_difference_gradient(f, initial, eps=1e-6):\n",
    "    # initial = initial position\n",
    "    # eps = epsilon\n",
    "    initial = np.array(initial, dtype=float)\n",
    "    n = len(initial)\n",
    "    output = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        ei = np.zeros(n)\n",
    "        ei[i] = 1\n",
    "        f1 = f(initial + eps * ei)\n",
    "        f2 = f(initial - eps * ei)\n",
    "        output[i] = (f1-f2)/(2*eps)\n",
    "    output = output.reshape(n,1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90e4f4-b17e-4081-b541-859b4fbc27ca",
   "metadata": {},
   "source": [
    "## Question 1 (Lasso regression)\n",
    "\n",
    "\n",
    "In this question, we are going to investigate Lasso regression. You are going to implement a gradient descent based learning algorithm for Lasso. Then use the implemented algorithm to find out what features are truely relevant in predicting the target. \n",
    "\n",
    "The dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}$ is imported below for you:\n",
    "* the input design matrix `d1X` contains ``n=200`` observations and each $\\mathbf{x}^{(i)}$ has ``m=200`` features \n",
    "* and the last column is the regression targets ${y}^{(i)}$ (and they are stored in `d1Y`)\n",
    "* among the 200 features, however, only three of them are relevant to the target $y$; and the rest 197 features are random noises\n",
    "* you may assume the bias term is zero for this question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742daef3-9b32-4027-801c-8786ae6e4166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.736198Z",
     "iopub.status.busy": "2025-02-13T12:59:16.735755Z",
     "iopub.status.idle": "2025-02-13T12:59:16.792412Z",
     "shell.execute_reply": "2025-02-13T12:59:16.791137Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.736172Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in dataset1\n",
    "dataset1_df = pd.read_csv('./datasets/dataset1.csv', header=0)\n",
    "dataset1 = np.array(dataset1_df)\n",
    "d1X, d1Y = dataset1[:, 0:200], dataset1[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3e131-5ea6-45b7-b937-edc67a37ff7d",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "\n",
    "Recall that Lasso regression's loss function is defined as  \n",
    "\n",
    "\n",
    "$$L(\\mathbf{w}) = \\frac{1}{2n} \\sum_{i=1}^n (y^{(i)} - \\mathbf{w}^{\\top}\\mathbf{x}^{(i)})^2 + \\lambda \\sum_{j=1}^m |w_j|,$$\n",
    "where $\\lambda >0$ is the penalty coefficient\n",
    "\n",
    "* give the gradient expression for $\\mathbf{w}$\n",
    "* then implement a gradient descent based algorithm to learn the parameter\n",
    "\n",
    "*Hint: To deal with sub-gradient descent properly, you may need to do the following:* \n",
    "1. *use a diminishing learning rate, e.g.* $\\gamma_t = \\frac{\\gamma}{\\sqrt{t}}$\n",
    "2. *if any weight switches signs during the learning process, set the weight to zero directly*\n",
    "3. *soft-thresholding, set a small constant e.g. $\\epsilon = 10^{-5}$, if any weight drops below $\\epsilon$, set it to zero directly*;\n",
    "*You can do either 2 or 3 (or both).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d3a22",
   "metadata": {},
   "source": [
    "#### Answer Task 1.1\n",
    "The gradient expression for $\\mathbf{w}$ $$\\large\n",
    "\t\\mathbf{w}_{Lasso} = (\\mathbf{X}^\\top\\mathbf{X} + n\\lambda \\mathbf{sign(w)})^{-1} \\mathbf{X}^\\top\\mathbf{y}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a gradient descent algorithm\n",
    "#np.sign handles sign stuff\n",
    "# apparently because this is a subgradient i need to solve it in iterative approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24a212-6595-4d0b-acd0-f1d66ff4555a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T17:49:06.560372Z",
     "iopub.status.busy": "2025-02-10T17:49:06.559407Z",
     "iopub.status.idle": "2025-02-10T17:49:06.576490Z",
     "shell.execute_reply": "2025-02-10T17:49:06.575074Z",
     "shell.execute_reply.started": "2025-02-10T17:49:06.560323Z"
    }
   },
   "source": [
    "### Task 1.2 \n",
    "\n",
    "A special property of Lasso regression is sparsity. That means, if a proper penalty parameter is used, irrelevant input features' parameters will become zero. And this sparsity property can be used to find out which input features are important or relevant. In this task, you are going to use the algorithm implemented in Task 2.1 to investigate which three of the input dimensions are truly relevant in terms of predicting the targets $y$. \n",
    "\n",
    "* plot the full regularisation path of $\\hat{\\mathbf{w}}(\\lambda)$ for a range of penalty parameter $\\lambda$\n",
    "\n",
    "\n",
    "* use the plot to tell which features are relevant and what are their cooresponding weights?\n",
    "\n",
    "*Hint:*\n",
    "1. *To find out the relevant features, you should try different $\\lambda$s and find the whole regularisation path; $\\ln\\lambda \\in [-8:0]$ seems appropriate*;\n",
    "2. *It is a good idea to fit Lasso with $\\lambda$s in an ascending order; since we know as $\\lambda$ increases, more weight $\\hat{\\mathbf{w}}(\\lambda_t)$ will become sparse*; \n",
    "3. *Instead of initialising the weight randomly for the gradient descent, you may \"warm start\" the algorithm with the learnt parameter with a smaller $\\lambda$;*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a88f75-d6e7-425d-8e42-dd19034e1c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.794408Z",
     "iopub.status.busy": "2025-02-13T12:59:16.793974Z",
     "iopub.status.idle": "2025-02-13T12:59:16.799528Z",
     "shell.execute_reply": "2025-02-13T12:59:16.798382Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.794362Z"
    }
   },
   "outputs": [],
   "source": [
    "### report your results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211324e",
   "metadata": {},
   "source": [
    "## Question 2 (Logistic regression)\n",
    "\n",
    "In this question, we are going to implement a logistic regression model to do binary classification on a simulated dataset. The dataset's input feature are four-dimensional vectors $\\mathbf{x}^{(i)} \\in \\mathbb{R}^4$ and as expected the targets are binary, *i.e.* $y^{(i)} \\in \\{0, 1\\}$. \n",
    "\n",
    "\n",
    "The dataset $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}$ is imported below for you:\n",
    "* ``dataset2``: 2000 observations and each input $\\mathbf{x}$ has 4 features \n",
    "* and the last column is the target ${y}^{(i)}$\n",
    "* the dataset is then split into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34706afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.801345Z",
     "iopub.status.busy": "2025-02-13T12:59:16.800911Z",
     "iopub.status.idle": "2025-02-13T12:59:16.830185Z",
     "shell.execute_reply": "2025-02-13T12:59:16.829106Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.801302Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in dataset2\n",
    "dataset2_df = pd.read_csv('./datasets/dataset2.csv', header=0)\n",
    "dataset2 = np.array(dataset2_df)\n",
    "d2X, d2Y = dataset2[:, 0:4], dataset2[:, -1]\n",
    "# split the data into training and testing \n",
    "# the training dataset has the first 1500 observation; \n",
    "# in practice, you should randomly shuffle before the split\n",
    "d2_xtrain, d2_ytrain = d2X[0:1500, :], d2Y[0:1500]\n",
    "# the testing dataset has the last 500\n",
    "d2_xtest, d2_ytest = d2X[1500:, :], d2Y[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270ebbf",
   "metadata": {},
   "source": [
    "### Task 2.1 Implementation of logistic regression\n",
    "\n",
    "Your task here is to implement a gradient descent based algorithm to train a logistic regression model. For this task, you cannot use `autograd`'s auto-differentiation method (*i.e.* the imported `grad` method). You will be guided to finish the task step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c28469",
   "metadata": {},
   "source": [
    "First, implement the `sigmoid` function:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77df7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.834657Z",
     "iopub.status.busy": "2025-02-13T12:59:16.834174Z",
     "iopub.status.idle": "2025-02-13T12:59:16.841827Z",
     "shell.execute_reply": "2025-02-13T12:59:16.840742Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.834606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9525741268224334)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2670fb",
   "metadata": {},
   "source": [
    "Second, implement the cross-entropy loss and its gradient. You may want to refer to the lecture slides for the details. Recall the binary **C**ross **E**ntropy (CE) _loss_ is \n",
    "\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b)=  -\\frac{1}{n}\\sum_{i=1}^n {y^{(i)}} \\ln \\sigma^{(i)}+ (1- y^{(i)}) \\ln (1-\\sigma^{(i)})\n",
    "$$\n",
    "\n",
    "where $\\sigma^{(i)} =\\sigma(\\mathbf{w}^\\top\\mathbf{x}^{(i)} + b).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d5057",
   "metadata": {},
   "source": [
    "### 2.1 Answer. \n",
    "The loss in matrix form is  $$J(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\frac{1}{n}\\left[\\,\\mathbf{y}^{T}\\ln(\\sigma) + (\\mathbf{1}-\\mathbf{y})^{T}\\ln(\\mathbf{1}-\\sigma)\\,\\right]$$\n",
    "\n",
    "The gradient for cross entropy loss is $$\\large\n",
    "\\nabla \\ell(\\mathbf{w}) =-\\frac{1}{n} \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\sigma})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da95f946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.843614Z",
     "iopub.status.busy": "2025-02-13T12:59:16.843164Z",
     "iopub.status.idle": "2025-02-13T12:59:16.860277Z",
     "shell.execute_reply": "2025-02-13T12:59:16.859297Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.843566Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_gradient(w, b, X, y):\n",
    "    # Number of samples\n",
    "    n = X.shape[1] \n",
    "    sigma = sigmoid(w.T @ X + b)\n",
    "    print(sigma.shape)\n",
    "    ## compute the loss \n",
    "    error = y.T @ np.log(sigma) + (1-y).T @ np.log(1 - sigma)\n",
    "    loss = - (1/n) * (np.sum(error)) \n",
    "\t## compute the gradient w.r.t w and b\n",
    "    print(X.shape)\n",
    "    gradient = - (1/n) * (X @ (y - sigma).T)\n",
    "\t## return the loss and the required gradients\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7c0b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(2, 3)\n",
      "Cross Entropy Loss: 2.10742672157239\n"
     ]
    }
   ],
   "source": [
    "# Test cross entropy loss function\n",
    "# Test parameters\n",
    "w = np.array([[0.5], [-0.5]])   # shape (2,1)\n",
    "b = np.array([[0.0]])           # shape (1,1)\n",
    "X = np.array([[1.0, -1.0, 2.0],\n",
    "              [0.5,  0.0, 1.0]])  # shape (2,3)\n",
    "y = np.array([[1, 0, 1]])       # shape (1,3)\n",
    "\n",
    "# Test your function\n",
    "loss, gradient = cross_entropy_loss_with_gradient(w, b, X, y)\n",
    "\n",
    "print(\"Cross Entropy Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafe409",
   "metadata": {},
   "source": [
    "Now, implement the gradient descent algorithm below. Before that, you should consider testing our gradient implementation before using it in the training algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8f29df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.862122Z",
     "iopub.status.busy": "2025-02-13T12:59:16.861659Z",
     "iopub.status.idle": "2025-02-13T12:59:16.878599Z",
     "shell.execute_reply": "2025-02-13T12:59:16.877610Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.862072Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression_train(X, y, lr, tol= 1e-5, maxIters= 2000):\n",
    "    n, d = X.shape \n",
    "    # initialise w0, b0\n",
    "    # w0 = np.zeros(d)\n",
    "    # b0 = 0.0\n",
    "    losses = []\n",
    "    # loop until converge\n",
    "    for i in range(maxIters):\n",
    "        ## Implement gradient descent here\n",
    "        w0 = w0\n",
    "        # Check convergence here \n",
    "        # if True:\n",
    "        #    break\n",
    "    return w0, b0, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e01f5",
   "metadata": {},
   "source": [
    "After you finish implementing all the above methods, use your learning algorithm to train a logistic regression model on the training dataset and answer the following questions:\n",
    "\n",
    "* plot the learning curve\n",
    "* report the learnt parameter with a learning rate 0.1, `tol=1e-5` and `maxIters=2000`\n",
    "* report the classification accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a1c3bd-ce1c-4121-a606-523ef126e9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T12:59:16.880433Z",
     "iopub.status.busy": "2025-02-13T12:59:16.879977Z",
     "iopub.status.idle": "2025-02-13T12:59:16.895887Z",
     "shell.execute_reply": "2025-02-13T12:59:16.894896Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.880387Z"
    }
   },
   "outputs": [],
   "source": [
    "## run your algorithm and report your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5eeddc",
   "metadata": {},
   "source": [
    "### Task 2.2 Regularisation\n",
    "\n",
    "In this sub-task, you are going to apply $L_2$ regularisation to the logistic regression model. The regularised loss is\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b)= - \\frac{1}{n}\\sum_{i=1}^n {y^{(i)}} \\ln \\sigma^{(i)}+ (1- y^{(i)}) \\ln (1-\\sigma^{(i)}) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|_2^2\n",
    "$$\n",
    "\n",
    "* where $\\lambda >0$ is the regularisation hyperparameter\n",
    "\n",
    "* note that we do not usually apply penalty on the bias parameter $b$\n",
    "\n",
    "Implement the following method that fits a regularised logistic regression model with a given $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9362f937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-13T13:00:09.728548Z",
     "iopub.status.busy": "2025-02-13T13:00:09.728002Z",
     "iopub.status.idle": "2025-02-13T13:00:09.738076Z",
     "shell.execute_reply": "2025-02-13T13:00:09.736312Z",
     "shell.execute_reply.started": "2025-02-13T13:00:09.728497Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression_reg_train(X, y, lr, lam = 0.01, tol= 1e-5, maxIters= 2000):\n",
    "    n, d = X.shape \n",
    "    # initialise w0, b0\n",
    "    w0 = np.zeros(d)\n",
    "    b0 = 0.0\n",
    "    losses = []\n",
    "    # loop until converge\n",
    "    # for i in range(maxIters):\n",
    "        \n",
    "    # return w0, b0, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe2830",
   "metadata": {},
   "source": [
    "Complete and report the following two results\n",
    "* report the training loss and learnt parameter by setting $\\lambda=0.01$\n",
    "* report the testing performance for the regularised logistic regression model with $\\lambda=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "034441f1-227d-4c3a-825e-7aea11c78769",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-13T12:59:16.917406Z",
     "iopub.status.idle": "2025-02-13T12:59:16.918031Z",
     "shell.execute_reply": "2025-02-13T12:59:16.917737Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.917707Z"
    }
   },
   "outputs": [],
   "source": [
    "## run your algorithm and report your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351e0f1-5105-416a-afd4-c33150a38809",
   "metadata": {},
   "source": [
    "### Task 2.3 Newton's method (extension)\n",
    "\n",
    "For convex loss functions, Newton's method converges much faster than a simple gradient descent algorithm. Implement a learning algorithm for the regularised logsitic regression with Newton's method. You are allowed to use auto-diff to finish this task.\n",
    "\n",
    "\n",
    "* use Newton's method to find the same logistic regression model\n",
    "\n",
    "* compare with gradient descent's learning curve, what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db43f0bf-23aa-42c2-8b58-de318942586e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-13T12:59:16.919964Z",
     "iopub.status.idle": "2025-02-13T12:59:16.920580Z",
     "shell.execute_reply": "2025-02-13T12:59:16.920293Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.920262Z"
    }
   },
   "outputs": [],
   "source": [
    "## run Newton's method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23535e1f-0d7f-46cc-ab1f-691f66f48ebe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-13T12:59:16.922507Z",
     "iopub.status.idle": "2025-02-13T12:59:16.923525Z",
     "shell.execute_reply": "2025-02-13T12:59:16.923226Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.923195Z"
    }
   },
   "outputs": [],
   "source": [
    "## report your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07610797-58a1-4cea-9079-00c76ac10c17",
   "metadata": {},
   "source": [
    "### Task 2.4 Weighted logistic regression (extension)\n",
    "\n",
    "Weighted logistic regression is a variant of the traditional logistic regression. It is usually used when the classification dataset is imbalanced. By assigning higher weights to the minority class and lower weights to the majority class, the model is encouraged to pay more attention to the minority class.\n",
    "\n",
    "Specifically, each training instance $y^{(i)}$ is given a positive weight $r^{(i)}$, and the weighted cross entropy loss becomes \n",
    "\n",
    "$$L(\\mathbf{w}, b) = - \\frac{1}{\\sum_{i=1}^n r^{(i)}} \\sum_{i=1}^n r^{(i)}\\cdot \\left ( {y^{(i)}} \\ln \\sigma^{(i)}+ (1- y^{(i)}) \\ln (1-\\sigma^{(i)}) \\right ) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "We can for example use the relative frequency of the training data to set $r^{(i)}$. Let $n^+ = \\sum_{i=1}^n y^{(i)}$ and $n^- = n - n^+$ be the number of positive and negative training instances respectively in the training data. The weights can be set as \n",
    "\n",
    "$$r^{(i)} = \\begin{cases}\\frac{n}{n^-} & y^{(i)} = 0 \\\\ \\frac{n}{n^+} & y^{(i)} =1\\end{cases}$$\n",
    "\n",
    "\n",
    "\n",
    "* derive and write down the gradient of the weighted loss w.r.t the learning parameter\n",
    "\n",
    "* implement a suitable training algorithm to learn the parameter\n",
    "\n",
    "* report the learnt parameter with $\\lambda =0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74f6fbd-0103-4f4e-a803-2870aefa7708",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-13T12:59:16.925015Z",
     "iopub.status.idle": "2025-02-13T12:59:16.925623Z",
     "shell.execute_reply": "2025-02-13T12:59:16.925339Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.925310Z"
    }
   },
   "outputs": [],
   "source": [
    "## gradient expression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f452c5-9a7c-4be1-9775-16320c4f46ff",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-13T12:59:16.927548Z",
     "iopub.status.idle": "2025-02-13T12:59:16.928505Z",
     "shell.execute_reply": "2025-02-13T12:59:16.928180Z",
     "shell.execute_reply.started": "2025-02-13T12:59:16.928151Z"
    }
   },
   "outputs": [],
   "source": [
    "## Implement and run your algorithm and report your findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
